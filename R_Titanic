setwd("~/Personal/OpenProject/Kaggle/Titanic")
train <- read.csv("~/Personal/OpenProject/Kaggle/Titanic/train.csv")
View(train)
test <- read.csv("~/Personal/OpenProject/Kaggle/Titanic/test.csv")
View(test)

#If we want to disable stirng as factors
#train <- read.csv("train.csv", stringsAsFactors=FALSE)

#Take a look on the structure of the dataframe train
str(train)
#To extract column from dataframe
train$Survived
#feed the vector to the function 
#table is summary statistics function
table(train$Survived)
#Now look for proportion - passing function o/p to other function
prop.table(table(train$Survived))

#Prediction starts
#Since the percentage 38% are perished.
#we will put our first prediction as all died in Test
test$Survived <- rep(0, 418)

#We need to submit passenger id and survived status to submit for kaggle
#data.frame command has created a new dataframe 
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
#Disable rownumber, because kaggle will reject the submission
write.csv(submit, file = "theyallperish.csv", row.names = FALSE)

#Prediction 2 
#saving First Woman and children in disaster
summary(train$Sex)
#two-way comparison on the number of males and females that survived
prop.table(table(train$Sex, train$Survived))
#So we need to tell the command to give us proportions 
#in the 1st dimension which stands for the rows
prop.table(table(train$Sex, train$Survived),1)
#Conditional variable value
test$Survived <- 0
test$Survived[test$Sex == 'female'] <- 1
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = "theyallperish.csv", row.names = FALSE)

#Prediction 3
#age variable
summary(train$Age)
#Our last few tables were on categorical variables, ie. they only had a few values.
#age variable is continuous variable which makes drawing proportion tables almost useless
#create new variable child
train$Child <- 0
train$Child[train$Age < 18] <- 1
#use aggregate for proportion in case of combination of 
#continous variables and categorial variables
#create a table with both gender and age to see the survival proportions for different subsets.
aggregate(Survived ~ Child + Sex, data=train, FUN=sum)
aggregate(Survived ~ Child + Sex, data=train, FUN=length)
aggregate(Survived ~ Child + Sex, data=train, FUN=function(x) {sum(x)/length(x)})
# Irrespective of child or not, Female are more survived based on previous command.
# we going to take a look on other two variables Class and FARE
# class is 3 values and Fare is continous variables. So We will convert fare to less levels
train$Fare2 <- '30+'
train$Fare2[train$Fare < 30 & train$Fare >= 20] <- '20-30'
train$Fare2[train$Fare < 20 & train$Fare >= 10] <- '10-20'
train$Fare2[train$Fare < 10] <- '<10'
#Aggregate again
aggregate(Survived ~ Fare2 + Pclass + Sex, data=train, FUN=function(x) {sum(x)/length(x)})
#class 3 women who paid more than $20 for their ticket actually also miss out on a lifeboat


test$Survived <- 0
test$Survived[test$Sex == 'female'] <- 1
test$Survived[test$Sex == 'female' & test$Pclass == 3 & test$Fare >= 20] <- 0
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = "theyallperish.csv", row.names = FALSE)

#Prediciton 4 - To improve and automate this process by decision trees
#Using CART Decision Tree Algorithm - Recursive Partionting and Regression Trees
library(rpart)

#Investigate other variables SibSp, Parch or Embarked
#skip passenger name, ticket number and cabin number are all unique identifiers for now
#If you wanted to predict a continuous variable, such as age, you may use method=”anova”
?rpart
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train, method="class")

#examine the tree
plot(fit)
text(fit)

# To get more informative graphics 
install.packages('rattle')
install.packages('rpart.plot')
install.packages('RColorBrewer')
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(fit)
# Male side, Age < 6 are survived , so go for submission
Prediction <- predict(fit, test, type = "class")
submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "myfirstdtree.csv", row.names = FALSE)

#Depth increase
#default
#The rpart package automatically caps the depth that the tree grows by using a metric called complexity 
?rpart.control
#cp - complexity parameter
#minsplit which governs how many passengers must sit in a bucket before even looking for a split
# Let’s max both out and 
#reduce cp to zero and minsplit to 2 (no split would obviously be possible for a single passenger in a bucket)
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train,
             method="class", control=rpart.control(minsplit=2, cp=0))
fancyRpartPlot(fit)

#above one is overfitting,example with additional parameters
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train,
             method="class", control=rpart.control( your controls ))
#to trim the tree
new.fit <- prp(fit,snip=TRUE)$obj
fancyRpartPlot(new.fit)


#Prediction 5 - Feature Engineering - To determine Predictive model is right or wrong
train$Name[1]
test$Survived <- NA
combi <- rbind(train, test)
combi$Name <- as.character(combi$Name)
#Data Cleansing
combi$Name[1]
strsplit(combi$Name[1], split='[,.]')
strsplit(combi$Name[1], split='[,.]')[[1]][2]
combi$Title <- sapply(combi$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][2]})
combi$Title <- sub(' ', '', combi$Title)
table(combi$Title)
combi$Title[combi$Title %in% c('Mme', 'Mlle')] <- 'Mlle'
combi$Title[combi$Title %in% c('Capt', 'Don', 'Major', 'Sir')] <- 'Sir'
combi$Title[combi$Title %in% c('Dona', 'Lady', 'the Countess', 'Jonkheer')] <- 'Lady'
#convert back to factor
combi$Title <- factor(combi$Title)
combi$FamilySize <- combi$SibSp + combi$Parch + 1
combi$Surname <- sapply(combi$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][1]})
combi$FamilyID <- paste(as.character(combi$FamilySize), combi$Surname, sep="")
combi$FamilyID[combi$FamilySize <= 2] <- 'Small'
table(combi$FamilyID)
famIDs <- data.frame(table(combi$FamilyID))
famIDs <- famIDs[famIDs$Freq <= 2,]
combi$FamilyID[combi$FamilyID %in% famIDs$Var1] <- 'Small'
combi$FamilyID <- factor(combi$FamilyID)

train <- combi[1:891,]
test <- combi[892:1309,]

fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize + FamilyID,
             data=train, method="class")
fancyRpartPlot(fit)

Prediction <- predict(fit, test, type = "class")
submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "myfutureengineering.csv", row.names = FALSE)

#Prediction 6 - Random Forest alogirthm 
#sampling example 
sample(1:10, replace = TRUE)
#Random Forest - supress NA value to work properly.

summary(combi$Age)
#Apply Anova - since Age is continous variable
Agefit <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + FamilySize,
                data=combi[!is.na(combi$Age),], method="anova")
combi$Age[is.na(combi$Age)] <- predict(Agefit, combi[is.na(combi$Age),])
summary(combi)

summary(combi$Embarked)
which(combi$Embarked == '')
combi$Embarked[c(62,830)] = "S"
combi$Embarked <- factor(combi$Embarked)
summary(combi$Fare)
which(is.na(combi$Fare))
combi$Fare[1044] <- median(combi$Fare, na.rm=TRUE)

#Random Forests in R can only digest factors with up to 32 levels. 
#Our FamilyID variable had almost double that. 

combi$FamilyID2 <- combi$FamilyID
combi$FamilyID2 <- as.character(combi$FamilyID2)
combi$FamilyID2[combi$FamilySize <= 3] <- 'Small'
combi$FamilyID2 <- factor(combi$FamilyID2)

train <- combi[1:891,]
test <- combi[892:1309,]

install.packages('randomForest')
library(randomForest)
# makes your results reproducible next time you load the code up, 
#otherwise you can get different classifications for each run
#The number inside isn’t important, you just need to ensure 
#you use the same seed number each time so that the same random numbers 
#are generated inside the Random Forest function.


set.seed(415)

fit <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize +
                      FamilyID2, data=train, importance=TRUE, ntree=2000)
varImpPlot(fit)

Prediction <- predict(fit, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "firstforest.csv", row.names = FALSE)

#Prediction 7 - Let’s try a forest of conditional inference trees
#They make their decisions in slightly different ways, 
#using a statistical test rather than a purity measure, 
#but the basic construction of each tree is fairly similar.

install.packages('party')
library(party)

#Conditional inference trees are able to handle factors with more levels than Random Forests can

set.seed(415)
fit <- cforest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize + FamilyID,
               data = train, controls=cforest_unbiased(ntree=2000, mtry=3))

Prediction <- predict(fit, test, OOB=TRUE, type = "response")
submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "firstforestcondition.csv", row.names = FALSE)
